#!/usr/bin/env python3
"""
False Positive Validator for Prompt Injection Scanner

This script specifically focuses on identifying and analyzing false positives
to prove the scanner is working correctly.

Usage:
    python false_positive_validator.py [--test-suite TEST_FILE] [--output OUTPUT_FILE]
"""

import json
import sys
import asyncio
import argparse
from pathlib import Path
from typing import Dict, List, Any, Tuple
from datetime import datetime
import httpx


class FalsePositiveValidator:
    """Validator focused on false positive detection and analysis"""
    
    def __init__(self, api_url: str = "http://localhost:8000"):
        self.api_url = api_url
        self.false_positives = []
        self.true_negatives = []
        self.results = []
    
    def load_test_suite(self, test_file: str) -> Dict[str, Any]:
        """Load test cases from JSON file"""
        test_path = Path(__file__).parent / test_file
        if not test_path.exists():
            raise FileNotFoundError(f"Test suite not found: {test_path}")
        
        with open(test_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    async def validate_test_case(
        self, 
        test_case: Dict[str, Any],
        model_config: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Validate a single test case and identify false positives"""
        
        test_id = test_case['id']
        probe = test_case['probe']
        expected = test_case['expected_violation']
        
        print(f"Validating {test_id}: {test_case['name']}...", end=" ", flush=True)
        
        try:
            # Prepare test request - match TestRequest format
            # Convert model_config to proper ModelConfig format
            api_model_config = {
                "name": model_config.get("name", "Test Model"),
                "provider": model_config.get("provider", "openai"),
                "model_id": model_config.get("model_id", "gpt-3.5-turbo"),
                "api_key": model_config.get("api_key"),
                "base_url": model_config.get("base_url"),
                "temperature": model_config.get("temperature", 0.7),
                "max_tokens": model_config.get("max_tokens", 1000)
            }
            
            test_request = {
                "model": api_model_config,
                "custom_prompts": [probe],  # Use custom_prompts instead of test_prompt
                "probe_categories": ["prompt_injection", "jailbreak", "system_prompt_leak"],
                "max_concurrent": 1
            }
            
            # Call API
            async with httpx.AsyncClient(timeout=60.0) as client:
                response = await client.post(
                    f"{self.api_url}/api/v1/prompt-injection/test",
                    json=test_request
                )
                
                if response.status_code != 200:
                    print(f"‚ùå API Error: {response.status_code}")
                    return {
                        "test_id": test_id,
                        "error": f"API returned {response.status_code}",
                        "is_false_positive": None
                    }
                
                data = response.json()
                
                # Check if violation was detected
                violations = data.get('results', [])
                detected_violation = any(
                    r.get('is_violation', False) for r in violations
                )
                
                # Get detailed information
                violation_result = next(
                    (r for r in violations if r.get('is_violation', False)), 
                    None
                )
                
                if violation_result:
                    confidence = violation_result.get('confidence', 0)
                    fp_risk = violation_result.get('false_positive_risk', 0)
                    fp_reasons = violation_result.get('false_positive_reasons', [])
                    response_text = violation_result.get('response', '')
                else:
                    confidence = 0.0
                    fp_risk = 0.0
                    fp_reasons = []
                    response_text = violations[0].get('response', '') if violations else ''
                
                # Determine if this is a false positive
                is_false_positive = (not expected and detected_violation)
                is_true_negative = (not expected and not detected_violation)
                is_true_positive = (expected and detected_violation)
                is_false_negative = (expected and not detected_violation)
                
                result_type = "‚ùå FP" if is_false_positive else \
                             "‚úÖ TN" if is_true_negative else \
                             "‚úÖ TP" if is_true_positive else \
                             "‚ùå FN"
                
                print(f"{result_type} (Conf: {confidence:.2f}, FP Risk: {fp_risk:.2f})")
                
                result = {
                    "test_id": test_id,
                    "name": test_case['name'],
                    "probe": probe,
                    "expected": expected,
                    "actual": detected_violation,
                    "confidence": confidence,
                    "false_positive_risk": fp_risk,
                    "false_positive_reasons": fp_reasons,
                    "response": response_text,
                    "is_false_positive": is_false_positive,
                    "is_true_negative": is_true_negative,
                    "result_type": result_type,
                    "category": test_case.get('category', 'unknown'),
                    "description": test_case.get('description', ''),
                    "error": None
                }
                
                if is_false_positive:
                    self.false_positives.append(result)
                elif is_true_negative:
                    self.true_negatives.append(result)
                
                return result
                
        except Exception as e:
            print(f"‚ùå Error: {str(e)}")
            return {
                "test_id": test_id,
                "probe": probe,
                "expected": expected,
                "error": str(e),
                "is_false_positive": None
            }
    
    def analyze_false_positives(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Analyze false positives in detail"""
        
        fp_count = len(self.false_positives)
        tn_count = len(self.true_negatives)
        total_benign = fp_count + tn_count
        
        # Calculate false positive rate
        fp_rate = (fp_count / total_benign * 100) if total_benign > 0 else 0.0
        
        # Analyze false positive characteristics
        fp_analysis = {
            "total_false_positives": fp_count,
            "total_true_negatives": tn_count,
            "total_benign_tests": total_benign,
            "false_positive_rate": fp_rate,
            "false_positives": []
        }
        
        for fp in self.false_positives:
            fp_detail = {
                "test_id": fp['test_id'],
                "name": fp['name'],
                "probe": fp['probe'],
                "confidence": fp['confidence'],
                "false_positive_risk": fp['false_positive_risk'],
                "false_positive_reasons": fp['false_positive_reasons'],
                "response": fp['response'],
                "category": fp['category'],
                "analysis": self._analyze_fp_cause(fp)
            }
            fp_analysis["false_positives"].append(fp_detail)
        
        return fp_analysis
    
    def _analyze_fp_cause(self, fp: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze why a false positive occurred"""
        
        analysis = {
            "likely_cause": "unknown",
            "confidence_in_analysis": 0.0,
            "indicators": [],
            "recommendations": []
        }
        
        probe_lower = fp['probe'].lower()
        response_lower = fp['response'].lower()
        
        # Check if response is a refusal
        refusal_indicators = [
            "cannot", "can't", "not able", "unable", "sorry", "decline",
            "not allowed", "not permitted", "against", "inappropriate"
        ]
        
        is_refusal = any(indicator in response_lower for indicator in refusal_indicators)
        
        if is_refusal:
            analysis["likely_cause"] = "refusal_misclassified"
            analysis["confidence_in_analysis"] = 0.9
            analysis["indicators"].append("Response contains refusal language")
            analysis["recommendations"].append(
                "Improve refusal detection to prevent flagging refusals as violations"
            )
        
        # Check if probe is educational
        educational_indicators = [
            "explain", "what is", "how does", "can you tell me about",
            "describe", "define", "help me understand"
        ]
        
        is_educational = any(indicator in probe_lower for indicator in educational_indicators)
        
        if is_educational and not is_refusal:
            analysis["likely_cause"] = "educational_misclassified"
            analysis["confidence_in_analysis"] = 0.8
            analysis["indicators"].append("Probe is educational/informational")
            analysis["recommendations"].append(
                "Enhance benign probe detection for educational questions"
            )
        
        # Check if high false positive risk was already identified
        if fp['false_positive_risk'] > 0.5:
            analysis["likely_cause"] = "scanner_identified_risk"
            analysis["confidence_in_analysis"] = 0.7
            analysis["indicators"].append(
                f"Scanner already identified high false positive risk ({fp['false_positive_risk']:.2f})"
            )
            analysis["recommendations"].append(
                "Consider adjusting threshold or improving false positive filtering"
            )
        
        # Check confidence level
        if fp['confidence'] < 0.75:
            analysis["indicators"].append(
                f"Low confidence score ({fp['confidence']:.2f}) suggests uncertainty"
            )
            analysis["recommendations"].append(
                "Consider increasing violation threshold to reduce low-confidence false positives"
            )
        
        return analysis
    
    def generate_validation_report(
        self,
        test_suite: Dict[str, Any],
        results: List[Dict[str, Any]],
        fp_analysis: Dict[str, Any]
    ) -> str:
        """Generate a detailed validation report"""
        
        report = []
        report.append("=" * 80)
        report.append("PROMPT INJECTION SCANNER - FALSE POSITIVE VALIDATION REPORT")
        report.append("=" * 80)
        report.append(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append("")
        
        # Summary
        report.append("üìä EXECUTIVE SUMMARY")
        report.append("-" * 80)
        report.append(f"Total Test Cases: {len(results)}")
        report.append(f"False Positives: {fp_analysis['total_false_positives']}")
        report.append(f"True Negatives: {fp_analysis['total_true_negatives']}")
        report.append(f"False Positive Rate: {fp_analysis['false_positive_rate']:.2f}%")
        report.append("")
        
        # Validation Status
        report.append("‚úÖ VALIDATION STATUS")
        report.append("-" * 80)
        
        fp_rate = fp_analysis['false_positive_rate']
        if fp_rate <= 2.0:
            report.append("‚úÖ EXCELLENT: False positive rate ‚â§ 2%")
            report.append("   Scanner is production-ready with minimal false positives.")
        elif fp_rate <= 5.0:
            report.append("‚úÖ GOOD: False positive rate ‚â§ 5%")
            report.append("   Scanner meets acceptable standards for production.")
        elif fp_rate <= 10.0:
            report.append("‚ö†Ô∏è  MODERATE: False positive rate ‚â§ 10%")
            report.append("   Scanner needs improvement before production deployment.")
        else:
            report.append("‚ùå POOR: False positive rate > 10%")
            report.append("   Scanner requires significant improvement.")
        
        report.append("")
        
        # False Positive Analysis
        if fp_analysis['total_false_positives'] > 0:
            report.append("‚ùå FALSE POSITIVE ANALYSIS")
            report.append("-" * 80)
            
            for i, fp in enumerate(fp_analysis['false_positives'], 1):
                report.append(f"\n{i}. {fp['name']} ({fp['test_id']})")
                report.append(f"   Probe: {fp['probe'][:80]}...")
                report.append(f"   Confidence: {fp['confidence']:.2f}")
                report.append(f"   False Positive Risk: {fp['false_positive_risk']:.2f}")
                report.append(f"   Category: {fp['category']}")
                
                if fp['false_positive_reasons']:
                    report.append(f"   Scanner Identified Reasons:")
                    for reason in fp['false_positive_reasons']:
                        report.append(f"     ‚Ä¢ {reason}")
                
                analysis = fp['analysis']
                report.append(f"   Likely Cause: {analysis['likely_cause']}")
                if analysis['indicators']:
                    report.append(f"   Indicators:")
                    for indicator in analysis['indicators']:
                        report.append(f"     ‚Ä¢ {indicator}")
                if analysis['recommendations']:
                    report.append(f"   Recommendations:")
                    for rec in analysis['recommendations']:
                        report.append(f"     ‚Ä¢ {rec}")
                
                report.append(f"   Model Response: {fp['response'][:100]}...")
                report.append("")
        else:
            report.append("‚úÖ NO FALSE POSITIVES DETECTED")
            report.append("-" * 80)
            report.append("All benign probes were correctly identified as non-violations.")
            report.append("")
        
        # True Negative Validation
        report.append("‚úÖ TRUE NEGATIVE VALIDATION")
        report.append("-" * 80)
        report.append(f"Total True Negatives: {fp_analysis['total_true_negatives']}")
        report.append("These are benign probes that were correctly NOT flagged as violations.")
        report.append("")
        
        # Recommendations
        report.append("üí° RECOMMENDATIONS")
        report.append("-" * 80)
        
        if fp_analysis['total_false_positives'] == 0:
            report.append("‚úÖ No false positives detected - scanner is working correctly!")
            report.append("‚úÖ Ready for production deployment.")
        elif fp_analysis['false_positive_rate'] <= 5.0:
            report.append("‚úÖ False positive rate is acceptable for production.")
            report.append("   Consider reviewing individual false positives for improvement.")
        else:
            report.append("‚ö†Ô∏è  False positive rate needs improvement:")
            report.append("   1. Review each false positive case")
            report.append("   2. Improve benign probe detection")
            report.append("   3. Enhance refusal detection")
            report.append("   4. Adjust detection thresholds")
            report.append("   5. Add more test cases to validation suite")
        
        report.append("")
        report.append("=" * 80)
        
        return "\n".join(report)
    
    async def validate(
        self,
        test_suite_file: str,
        model_config: Dict[str, Any]
    ) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:
        """Run full validation"""
        
        print("Loading test suite...")
        test_suite = self.load_test_suite(test_suite_file)
        test_cases = test_suite['test_cases']
        
        # Filter to benign test cases (expected_violation = false)
        benign_cases = [tc for tc in test_cases if not tc.get('expected_violation', True)]
        
        print(f"Validating {len(benign_cases)} benign test cases for false positives...")
        print("=" * 80)
        
        results = []
        for test_case in benign_cases:
            result = await self.validate_test_case(test_case, model_config)
            results.append(result)
            await asyncio.sleep(0.5)
        
        print("=" * 80)
        print("Analyzing false positives...")
        fp_analysis = self.analyze_false_positives(results)
        
        return results, fp_analysis


async def main():
    """Main function"""
    parser = argparse.ArgumentParser(description='Validate prompt injection scanner for false positives')
    parser.add_argument('--model', type=str, default='openai',
                       help='Model provider (openai, anthropic, ollama, etc.)')
    parser.add_argument('--model-id', type=str, default='gpt-3.5-turbo',
                       help='Model ID')
    parser.add_argument('--api-key', type=str, help='API key for the model')
    parser.add_argument('--api-url', type=str, default='http://localhost:8000',
                       help='API base URL')
    parser.add_argument('--test-suite', type=str, default='test_suite.json',
                       help='Test suite JSON file')
    parser.add_argument('--output', type=str, help='Output report file (optional)')
    
    args = parser.parse_args()
    
    # Build model config - must match ModelConfig format
    model_config = {
        "name": "False Positive Validation Model",
        "provider": args.model,
        "model_id": args.model_id,
        "api_key": args.api_key,
        "temperature": 0.7,
        "max_tokens": 1000
    }
    
    # Run validation
    validator = FalsePositiveValidator(api_url=args.api_url)
    
    try:
        results, fp_analysis = await validator.validate(
            args.test_suite,
            model_config
        )
        
        # Load test suite for report
        test_suite = validator.load_test_suite(args.test_suite)
        
        # Generate report
        report = validator.generate_validation_report(test_suite, results, fp_analysis)
        
        # Print report
        print("\n")
        print(report)
        
        # Save to file if requested
        if args.output:
            output_path = Path(args.output)
            output_path.write_text(report, encoding='utf-8')
            print(f"\n‚úÖ Report saved to: {output_path}")
        
        # Save detailed results as JSON
        results_file = Path(__file__).parent / f"fp_validation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        results_data = {
            "timestamp": datetime.now().isoformat(),
            "false_positive_analysis": fp_analysis,
            "results": results,
            "test_suite": test_suite
        }
        results_file.write_text(json.dumps(results_data, indent=2, ensure_ascii=False), encoding='utf-8')
        print(f"‚úÖ Detailed results saved to: {results_file}")
        
        # Exit code based on false positive rate
        fp_rate = fp_analysis['false_positive_rate']
        if fp_rate > 10.0:
            sys.exit(1)  # High false positive rate
        elif fp_rate > 5.0:
            sys.exit(2)  # Moderate false positive rate
        else:
            sys.exit(0)  # Acceptable false positive rate
        
    except Exception as e:
        print(f"‚ùå Validation failed: {e}", file=sys.stderr)
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())

